# Neural Network Fundamentals Problem Sheet

## Part 1: Derivations

### 1.1 Gradient of ReLU Function

**ReLU Definition:**
$$\text{ReLU}(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

**Derivative:**
$$\frac{d}{dx}\text{ReLU}(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \\ \text{undefined} & \text{if } x = 0 \end{cases}$$

In practice, we set the derivative at x = 0 to either 0 or 1 (commonly 0).

**Key Properties:**
- **Non-saturating**: Gradient is  (1) for positive inputs
- **Sparse activation**: Produces exactly zero for negative inputs
- **Computationally efficient**: Simple thresholding operation



### 1.2 Gradient of Sigmoid Function

**Sigmoid Definition:**
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**Derivative:**
$$\frac{d}{dx}\sigma(x) = \sigma(x)(1 - \sigma(x))$$

**Proof:**
$$\frac{d}{dx}\sigma(x) = \frac{d}{dx}\left(\frac{1}{1 + e^{-x}}\right)$$
$$= -\frac{1}{(1 + e^{-x})^2} \cdot (-e^{-x})$$
$$= \frac{e^{-x}}{(1 + e^{-x})^2}$$
$$= \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}}$$
$$= \sigma(x) \cdot \frac{e^{-x}}{1 + e^{-x}} = \sigma(x)(1 - \sigma(x))$$

**Key Properties:**
- **Maximum gradient**: 0.25 (at x = 0)
- **Vanishing gradient**: For |x| > 5, gradient approaches 0
- **Output range**: (0, 1), useful for probabilities



### 1.3 Loss Functions and Their Impact on Convergence

#### **Mean Squared Error (MSE) - Regression**

$$L_{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**Gradient:**
$$\frac{\partial L_{MSE}}{\partial \hat{y}_i} = -\frac{2}{n}(y_i - \hat{y}_i)$$

**Convergence Properties:**
- Linear gradient with respect to error
- Sensitive to outliers (squared term amplifies large errors)
- Smooth optimization landscape for small errors
- Can lead to slow convergence when predictions are very wrong



#### **Binary Cross-Entropy (BCE) - Binary Classification**

$$L_{BCE} = -\frac{1}{n}\sum_{i=1}^{n}\left[y_i\log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)\right]$$

**Gradient (with sigmoid output):**
$$\frac{\partial L_{BCE}}{\partial z_i} = \hat{y}_i - y_i$$

where z is the logit (input to sigmoid).

**Convergence Properties:**
- Larger gradients when predictions are confidently wrong
- Faster convergence than MSE for classification
- Well-suited for sigmoid outputs (natural pairing)
- Can suffer from numerical instability if outputs are exactly 0 or 1


#### **Categorical Cross-Entropy (CCE) - Multi-class Classification**

$$L_{CCE} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})$$

**Gradient (with softmax output):**
$$\frac{\partial L_{CCE}}{\partial z_{i,c}} = \hat{y}_{i,c} - y_{i,c}$$

**Convergence Properties:**
- Similar benefits to BCE for multi-class problems
- Encourages confident, correct predictions
- Works naturally with softmax activation




## Part 2: Conceptual Understanding

### 2.1 When to Use Which Activation Function

| Activation | Best Use Cases | Advantages | Disadvantages |
|------------|---------------|------------|---------------|
| **ReLU** | Hidden layers in deep networks, CNNs | Fast computation, no vanishing gradient for positive values, sparse activation | Dying ReLU problem (neurons can "die"), not zero-centered |
| **Leaky ReLU** | Deep networks prone to dying ReLU | Prevents dying neurons | Small negative slope may not be optimal |
| **Sigmoid** | Output layer for binary classification | Output in [0,1] for probabilities | Vanishing gradients, not zero-centered, slow convergence |
| **Tanh** | Hidden layers in RNNs, when zero-centered outputs needed | Zero-centered, stronger gradients than sigmoid | Still suffers from vanishing gradients |
| **Softmax** | Output layer for multi-class classification | Produces probability distribution | Only for output layer |
| **Linear** | Output layer for regression | Unbounded output range | No non-linearity (useless in hidden layers) |

**General Guidelines:**
- **Default choice**: ReLU for hidden layers
- **Deep networks**: Consider variants like Leaky ReLU, PReLU, or ELU
- **Output layers**: Match to task (sigmoid for binary, softmax for multi-class, linear for regression)
- **RNNs/LSTMs**: Tanh commonly used due to zero-centered outputs



### 2.2 Visualizing Underfitting vs Overfitting

#### **Underfitting (High Bias)**


**Characteristics:**
- it has a High training loss AND high validation loss
- it consists of Small gap between training and validation loss
- the Model is too simple to capture underlying patterns


---

#### **Good Fit (Optimal)**


**Characteristics:**
- the Low training loss AND low validation loss
- Small, acceptable gap between them
- Model generalizes well to unseen data

---

#### **Overfitting (High Variance)**


**Characteristics:**
- Low (or decreasing) training loss
- High (or increasing) validation loss
- Large gap between training and validation loss
- Model memorizes training data instead of learning patterns



