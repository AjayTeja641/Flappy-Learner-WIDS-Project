# Neural Network Fundamentals – Problem Sheet

## Part 1: Mathematical Derivations

### 1.1 Gradient of the ReLU Function

**Definition**
[
\text{ReLU}(x) = \max(0, x) =
\begin{cases}
x & \text{if } x > 0 \
0 & \text{if } x \leq 0
\end{cases}
]

**Derivative**
[
\frac{d}{dx}\text{ReLU}(x) =
\begin{cases}
1 & \text{if } x > 0 \
0 & \text{if } x < 0 \
\text{undefined} & \text{if } x = 0
\end{cases}
]

In practice, the derivative at (x = 0) is set to **0**.

**Key Properties**

* **Non-saturating:** Constant gradient of 1 for positive inputs
* **Sparse activation:** Outputs zero for negative inputs
* **Computationally efficient:** Simple threshold operation

---

### 1.2 Gradient of the Sigmoid Function

**Definition**
[
\sigma(x) = \frac{1}{1 + e^{-x}}
]

**Derivative**
[
\frac{d}{dx}\sigma(x) = \sigma(x)(1 - \sigma(x))
]

**Proof**
[
\frac{d}{dx}\left(\frac{1}{1 + e^{-x}}\right)
= \frac{e^{-x}}{(1 + e^{-x})^2}
= \sigma(x)(1 - \sigma(x))
]

**Key Properties**

* **Maximum gradient:** 0.25 at (x = 0)
* **Vanishing gradient:** Gradients approach zero for large |x|
* **Output range:** (0, 1), suitable for probabilities

---

### 1.3 Loss Functions and Their Impact on Convergence

#### Mean Squared Error (MSE) — Regression

[
L_{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2
]

**Gradient**
[
\frac{\partial L_{MSE}}{\partial \hat{y}_i} = -\frac{2}{n}(y_i - \hat{y}_i)
]

**Convergence Properties**

* Linear gradient with respect to error
* Sensitive to outliers
* Smooth optimization landscape
* Can converge slowly for large initial errors

---

#### Binary Cross-Entropy (BCE) — Binary Classification

[
L_{BCE} = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]
]

**Gradient (with sigmoid output)**
[
\frac{\partial L_{BCE}}{\partial z_i} = \hat{y}_i - y_i
]

**Convergence Properties**

* Strong gradients for confident but incorrect predictions
* Faster convergence than MSE for classification
* Numerically unstable if predictions reach 0 or 1

---

#### Categorical Cross-Entropy (CCE) — Multi-Class Classification

[
L_{CCE} = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C} y_{i,c}\log(\hat{y}_{i,c})
]

**Gradient (with softmax output)**
[
\frac{\partial L_{CCE}}{\partial z_{i,c}} = \hat{y}*{i,c} - y*{i,c}
]

**Convergence Properties**

* Encourages confident and correct predictions
* Well-suited for softmax-based classifiers

---

## Part 2: Conceptual Understanding

### 2.1 Choosing the Right Activation Function

| Activation     | Use Case            | Advantages                       | Disadvantages                 |
| -------------- | ------------------- | -------------------------------- | ----------------------------- |
| **ReLU**       | Hidden layers, CNNs | Fast, avoids vanishing gradients | Dying ReLU, not zero-centered |
| **Leaky ReLU** | Deep networks       | Prevents dead neurons            | Fixed negative slope          |
| **Sigmoid**    | Binary output layer | Probabilistic output             | Vanishing gradients           |
| **Tanh**       | RNN hidden layers   | Zero-centered                    | Vanishing gradients           |
| **Softmax**    | Multi-class output  | Probability distribution         | Output layer only             |
| **Linear**     | Regression output   | Unbounded output                 | No non-linearity              |

**Guidelines**

* Use **ReLU** as default for hidden layers
* Use **sigmoid** for binary classification outputs
* Use **softmax** for multi-class outputs
* Use **linear** activation for regression

---

### 2.2 Underfitting vs Overfitting

#### Underfitting (High Bias)

* High training loss
* High validation loss
* Small gap between losses
* Model too simple to capture patterns

---

#### Good Fit (Optimal)

* Low training loss
* Low validation loss
* Small, stable gap
* Good generalization

---

#### Overfitting (High Variance)

* Very low training loss
* Increasing validation loss
* Large gap between losses
* Model memorizes training data
