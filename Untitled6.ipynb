{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORiGkNcdvE65GM/9MrmVHW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjayTeja641/Flappy-Learner-WIDS-Project/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5fXYLifMeNw1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "GRID_SIZE = 5\n",
        "ACTIONS = {\n",
        "    0: (-1, 0),  # Up\n",
        "    1: (1, 0),   # Down\n",
        "    2: (0, -1),  # Left\n",
        "    3: (0, 1)    # Right\n",
        "}\n",
        "\n",
        "GOAL_STATE = (4, 4)\n",
        "GAMMA = 0.9\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def step(state, action):\n",
        "    if state == GOAL_STATE:\n",
        "        return state, 0\n",
        "\n",
        "    move = ACTIONS[action]\n",
        "    next_state = (\n",
        "        min(max(state[0] + move[0], 0), GRID_SIZE - 1),\n",
        "        min(max(state[1] + move[1], 0), GRID_SIZE - 1)\n",
        "    )\n",
        "\n",
        "    reward = 10 if next_state == GOAL_STATE else -1\n",
        "    return next_state, reward\n"
      ],
      "metadata": {
        "id": "NFeUbHm4eYuw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(theta=1e-4):\n",
        "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for i in range(GRID_SIZE):\n",
        "            for j in range(GRID_SIZE):\n",
        "                state = (i, j)\n",
        "                if state == GOAL_STATE:\n",
        "                    continue\n",
        "\n",
        "                v = V[i, j]\n",
        "                action_values = []\n",
        "\n",
        "                for action in ACTIONS:\n",
        "                    next_state, reward = step(state, action)\n",
        "                    action_values.append(\n",
        "                        reward + GAMMA * V[next_state]\n",
        "                    )\n",
        "\n",
        "                V[i, j] = max(action_values)\n",
        "                delta = max(delta, abs(v - V[i, j]))\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "_Nh3foNtecm6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_policy(V):\n",
        "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
        "\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            state = (i, j)\n",
        "            if state == GOAL_STATE:\n",
        "                continue\n",
        "\n",
        "            action_values = []\n",
        "            for action in ACTIONS:\n",
        "                next_state, reward = step(state, action)\n",
        "                action_values.append(reward + GAMMA * V[next_state])\n",
        "\n",
        "            policy[i, j] = np.argmax(action_values)\n",
        "\n",
        "    return policy\n"
      ],
      "metadata": {
        "id": "Ni06j1qpeg7S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_optimal = value_iteration()\n",
        "policy_optimal = extract_policy(V_optimal)\n",
        "\n",
        "print(\"Optimal Value Function:\\n\", V_optimal)\n",
        "print(\"\\nOptimal Policy:\\n\", policy_optimal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itbTjHSbejIp",
        "outputId": "f9f11045-8ce2-4b51-ddac-db693f255e84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Value Function:\n",
            " [[-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
            " [ 0.62882   1.8098    3.122     4.58      6.2     ]\n",
            " [ 1.8098    3.122     4.58      6.2       8.      ]\n",
            " [ 3.122     4.58      6.2       8.       10.      ]\n",
            " [ 4.58      6.2       8.       10.        0.      ]]\n",
            "\n",
            "Optimal Policy:\n",
            " [[1 1 1 1 1]\n",
            " [1 1 1 1 1]\n",
            " [1 1 1 1 1]\n",
            " [1 1 1 1 1]\n",
            " [3 3 3 3 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, gamma=0.9, theta=1e-3, max_iter=10):\n",
        "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        delta = 0\n",
        "        for i in range(GRID_SIZE):\n",
        "            for j in range(GRID_SIZE):\n",
        "                if (i, j) == GOAL_STATE:\n",
        "                    continue\n",
        "\n",
        "                action = policy[i, j]\n",
        "                next_state, reward = step((i, j), action)\n",
        "                new_v = reward + gamma * V[next_state]\n",
        "\n",
        "                delta = max(delta, abs(V[i, j] - new_v))\n",
        "                V[i, j] = new_v\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return V"
      ],
      "metadata": {
        "id": "J6k-YBV-ek1O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(V):\n",
        "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
        "    policy_stable = True\n",
        "\n",
        "    for i in range(GRID_SIZE):\n",
        "        for j in range(GRID_SIZE):\n",
        "            state = (i, j)\n",
        "            if state == GOAL_STATE:\n",
        "                continue\n",
        "\n",
        "            old_action = policy[i, j]\n",
        "            action_values = []\n",
        "\n",
        "            for action in ACTIONS:\n",
        "                next_state, reward = step(state, action)\n",
        "                action_values.append(reward + GAMMA * V[next_state])\n",
        "\n",
        "            best_action = np.argmax(action_values)\n",
        "            policy[i, j] = best_action\n",
        "\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "\n",
        "    return policy, policy_stable\n"
      ],
      "metadata": {
        "id": "9uYFj-r7exHI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration():\n",
        "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
        "\n",
        "    while True:\n",
        "        V = policy_evaluation(policy)\n",
        "        policy, stable = policy_improvement(V)\n",
        "\n",
        "        if stable:\n",
        "            break\n",
        "\n",
        "    return V, policy\n"
      ],
      "metadata": {
        "id": "vlvjNvG5ez3G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V_pi, policy_pi = policy_iteration()\n",
        "\n",
        "print(\"Policy Iteration Value Function:\\n\", V_pi)\n",
        "print(\"\\nPolicy Iteration Policy:\\n\", policy_pi)\n"
      ],
      "metadata": {
        "id": "bMk9iFIpe0qL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}